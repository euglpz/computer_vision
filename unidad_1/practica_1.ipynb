{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "# TUIA 2024 - COMPUTER VISION\n",
        "\n",
        "# PRÁCTICA UNIDAD 1\n",
        "\n",
        "Eugenio M. Lopez\n",
        "\n",
        "----\n",
        "\n"
      ],
      "metadata": {
        "id": "G6CWP-SzPMJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "izfUtkRqPE4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación del entorno.\n",
        "Acceso al repositorio de la unidad. Lo clonamos y cd al repo."
      ],
      "metadata": {
        "id": "8IZNSAVjU8V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_NAME = \"computer_vision\"\n",
        "if REPO_NAME not in os.getcwd():\n",
        "  if not os.path.exists(REPO_NAME):\n",
        "    !git clone https://github.com/euglpz/{REPO_NAME}.git\n",
        "  os.chdir(REPO_NAME)"
      ],
      "metadata": {
        "id": "UHI3c4a1U68c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Me muevo a la carpeta de la unidad 1\n",
        "os.chdir('unidad_1')"
      ],
      "metadata": {
        "id": "-a3opkXlXC2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1\n",
        "\n",
        "Elija una de las imágenes color que tomó para la clase y aplique separación de canales y elija un método para transformarla en escala de grises. Muestre por pantalla los resultados obtenidos."
      ],
      "metadata": {
        "id": "vmblJFkTPG7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmlbk-ZG9VLT"
      },
      "outputs": [],
      "source": [
        "# Cargar la imagen\n",
        "image = cv2.imread('img_simple.jpg')\n",
        "\n",
        "# Convertir la imagen a RGB\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separación de canales rgb (alto, ancho, canal)\n",
        "r, g, b = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]"
      ],
      "metadata": {
        "id": "QD9W5Uhh-iTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra de imágenes por canal\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(r,cmap='Reds')\n",
        "plt.title('R')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(g,cmap='Greens')\n",
        "plt.title('G')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(b, cmap='Blues')\n",
        "plt.title('B')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6jntJZL7-z1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir la imagen a escala de grises\n",
        "image_gr = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "KmBB3KzB_wVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imagen en escala de grises\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image_gr, cmap='Greys')"
      ],
      "metadata": {
        "id": "bY5m7M1q_28Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2\n",
        "Con las fotografías pedidas por la cátedra la clase pasada (la foto de objetos con fondo liso, y fotos del mismo producto en un contexto más complejo) usar los métodos de extracción de características (esos anteriores al Deep Learning) para encontrar la ubicación del producto dentro de la imagen."
      ],
      "metadata": {
        "id": "dHYxbx5udlj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo imagen\n",
        "img = cv2.imread('img_simple.jpg')\n",
        "\n",
        "# Escala de grises\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Inicializo objeto SIFT\n",
        "sift = cv2.SIFT_create()\n",
        "kp = sift.detect(gray, None) # Keypoints\n",
        "\n",
        "img_with_sift_kp = cv2.drawKeypoints(gray, kp, img)\n",
        "\n",
        "# Guardo la img con los kp\n",
        "cv2.imwrite('sift_keypoints.jpg', img)"
      ],
      "metadata": {
        "id": "OP5K4w2zBBvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usar matplotlib para mostrar las imágenes\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_with_sift_kp)\n",
        "plt.title('SIFT Keypoints')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.title('Imagen original ByN')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BMW0PIXwgJFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keypoint usando flag para dibujar circulos"
      ],
      "metadata": {
        "id": "ulxpv843pCN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_with_sift_kp_flag = cv2.drawKeypoints(gray, kp, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "cv2.imwrite('sift_keypoints_2.jpg',img)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_with_sift_kp_flag)\n",
        "plt.title('SIFT Keypoints')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.title('Imagen original ByN')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rjl2nG_cC-_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracción características de otra imagen"
      ],
      "metadata": {
        "id": "AQGZWT3vs9Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('img_objetos_detras.jpg')\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "sift = cv2.SIFT_create()\n",
        "kp = sift.detect(gray,None)\n",
        "\n",
        "img_2_with_sift_kp_flag = cv2.drawKeypoints(gray,kp,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "cv2.imwrite('sift_keypoints_3.jpg',img)\n"
      ],
      "metadata": {
        "id": "vCAjd-CaCv45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_2_with_sift_kp_flag)\n",
        "plt.title('SiFT keypoints')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.title('Img orig')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6_-a36dOt3S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keypoints matching entre ambas imágenes usando FLANN (Fast Library for Approximate Nearest Neighbors)"
      ],
      "metadata": {
        "id": "rhC4L0PJuqaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "img1 = cv.imread('img_simple.jpg', cv.IMREAD_GRAYSCALE)\n",
        "# img2 = cv.imread('img_tapada.jpg', cv.IMREAD_GRAYSCALE)\n",
        "img2 = cv.imread('img_objetos_detras_3.jpg', cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "if img1 is None or img2 is None:\n",
        "  print('Could not open or find the images!')\n",
        "  exit(0)\n",
        "\n",
        "# Primer paso: detección de keypoints usando SIFT y calculo de descriptores\n",
        "minHessian = 400\n",
        "detector = cv.SIFT_create()\n",
        "keypoints1, descriptors1 = detector.detectAndCompute(img1, None)\n",
        "keypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n",
        "\n",
        "# Segundo paso: matcheo de vectores descriptores usando FLANN\n",
        "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\n",
        "knn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n",
        "\n",
        "# Filtrar las coincidencias mediante Lowe ratio test\n",
        "ratio_thresh = 0.7\n",
        "good_matches = []\n",
        "for m,n in knn_matches:\n",
        "  if m.distance < ratio_thresh * n.distance:\n",
        "    good_matches.append(m)\n",
        "\n",
        "# Dibujar coincidencias\n",
        "img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\n",
        "cv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Mostrar coincidencias detectadas\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(img_matches, cmap='Greys')\n",
        "\n",
        "# Guardo imagen final\n",
        "cv.imwrite('finish.jpg',img_matches)\n",
        "\n",
        "cv.waitKey()"
      ],
      "metadata": {
        "id": "oajl878eEHkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4\n",
        "Con los videos de youtube.com de cámara fija pedidos para esta clase, aplicar los algoritmos de detección de movimiento vistos en la teoría"
      ],
      "metadata": {
        "id": "AhB4Bdxt563B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytube\n",
        "!apt update && apt install -y handbrake"
      ],
      "metadata": {
        "id": "V7H7kUpoVzhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# URL del video de YouTube\n",
        "url = 'https://www.youtube.com/watch?v=fUHmDs-a1Z8'\n",
        "\n",
        "# Crear un objeto YouTube\n",
        "yt = YouTube(url)\n",
        "\n",
        "# Seleccionar el stream con resolución de 720p\n",
        "video = yt.streams.filter(progressive=True, file_extension='mp4', res=\"720p\").first()\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "output_filename = 'video.mp4'\n",
        "\n",
        "# Descargar el video con el nombre especificado\n",
        "video.download(filename=output_filename)\n",
        "\n",
        "print(\"Descarga completada.\")"
      ],
      "metadata": {
        "id": "tgX6_umf-D87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#Esto convierte el video a un formato más liviano de trabajo\n",
        "!ffmpeg -y -i video.mp4 -vf \"scale=600:-1\" -an -t 30 video_600.mp4"
      ],
      "metadata": {
        "id": "XoNX8SSYIgWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba del video\n"
      ],
      "metadata": {
        "id": "MXBh2HDDNB9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "\n",
        "url = 'video_600.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "vGfjWmOEM5Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy as media  # Importar la biblioteca mediapy para manejo de medios\n",
        "import cv2  # Importar OpenCV para el procesamiento de imágenes y videos\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Función para oscurecer una imagen:\n",
        "def process_image(new_image, prev_image, **kwargs):\n",
        "    # Convertir la imagen a float32\n",
        "    new_image_float = new_image.astype(np.float32)\n",
        "\n",
        "    # Reducir el brillo de la imagen a la mitad\n",
        "    new_image_float *= 0.5\n",
        "\n",
        "    # Convertir la imagen de vuelta a uint8\n",
        "    new_image_uint8 = np.clip(new_image_float, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return new_image_uint8\n",
        "\n",
        "def draw_contours(frame, contours, color=(0, 255, 0), thickness=2):\n",
        "    # Comprobar si la imagen es en escala de grises (1 canal)\n",
        "    if len(frame.shape) == 2 or frame.shape[2] == 1:\n",
        "        # Convertir la imagen de escala de grises a color (3 canales)\n",
        "        result_image = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "    else:\n",
        "        # Si ya es una imagen de color, simplemente hacer una copia\n",
        "        result_image = frame.copy()\n",
        "\n",
        "    # Dibujar cada contorno en la imagen\n",
        "    for contour in contours:\n",
        "        # Obtener el rectángulo delimitador para cada contorno\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        # Dibujar el rectángulo\n",
        "        cv2.rectangle(result_image, (x, y), (x + w, y + h), color, thickness)\n",
        "\n",
        "    return result_image\n",
        "\n",
        "# Función para procesar un video:\n",
        "def video_processor(filename_in, filename_out, process_func, max_time=10, **kwargs):\n",
        "    # Abrir el video de entrada para lectura\n",
        "    with media.VideoReader(filename_in) as r:\n",
        "        # Crear un archivo de video de salida\n",
        "        with media.VideoWriter(filename_out, shape=r.shape, fps=r.fps, bps=r.bps) as w:\n",
        "            count = 0  # Inicializar contador de fotogramas\n",
        "            prev_image = None  # Inicializar la imagen previa\n",
        "\n",
        "            # Iterar sobre cada imagen (fotograma) del video\n",
        "            for image in r:\n",
        "                new_image = media.to_uint8(image)  # Convertir la imagen a formato flotante\n",
        "\n",
        "                # Comprobar si es la primera imagen\n",
        "                if prev_image is None:\n",
        "                    prev_image = new_image.copy()\n",
        "\n",
        "                # Procesar la imagen utilizando la función dada\n",
        "                processed_image = process_func(new_image, prev_image, **kwargs)\n",
        "\n",
        "                # Añadir la imagen procesada al video de salida\n",
        "                w.add_image(processed_image)\n",
        "\n",
        "                # Actualizar la imagen previa\n",
        "                prev_image = new_image.copy()\n",
        "\n",
        "                # Incrementar el contador de fotogramas\n",
        "                count += 1\n",
        "\n",
        "                # Detener el proceso si se alcanza el tiempo máximo\n",
        "                if count >= max_time * r.fps:\n",
        "                    break\n",
        "\n",
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_600.mp4'\n",
        "filename_out = 'video_dark.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_image, 10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "OZ4asAmpNoeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función actualizada para realizar diferencia de fotogramas con normalización:\n",
        "def process_frame_difference(new_image, prev_image, **kwargs):\n",
        "    # Convertir las imágenes a escala de grises\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
        "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
        "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
        "\n",
        "    # Normalizar la imagen de diferencia\n",
        "    norm_diff = cv2.normalize(frame_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Umbralizar la imagen para resaltar las diferencias\n",
        "    _, thresh = cv2.threshold(norm_diff, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Convertir la imagen umbralizada a color para mantener la consistencia con el video original\n",
        "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    return thresh_color\n",
        "\n",
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_600.mp4'\n",
        "filename_out = 'video_frame_difference.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_frame_difference, 10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "eqpajYf2PsbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función actualizada para detectar movimientos y dibujar cuadros delimitadores:\n",
        "def process_frame_difference_full(new_image, prev_image, **kwargs):\n",
        "    # Convertir las imágenes a escala de grises\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
        "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
        "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
        "\n",
        "    # Normalizar la imagen de diferencia\n",
        "    norm_diff = cv2.normalize(frame_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Umbralizar la imagen para resaltar las diferencias\n",
        "    _, thresh = cv2.threshold(norm_diff, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Dilatar la imagen umbralizada para mejorar la detección de contornos\n",
        "    kernel = np.ones((5,5),np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations = 1)\n",
        "\n",
        "    # Convertir la imagen dilatada a formato adecuado para findContours\n",
        "    dilated = dilated.astype(np.uint8)\n",
        "\n",
        "    # Encontrar contornos en la imagen dilatada\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Dibujar cuadros delimitadores alrededor de los contornos\n",
        "    if kwargs.get('draw_mode', 0) == 0:\n",
        "      result_image = draw_contours(new_image, contours)\n",
        "    elif kwargs.get('draw_mode', 0) == 1:\n",
        "      result_image = draw_contours(thresh, contours)\n",
        "\n",
        "    return result_image\n",
        "\n",
        "\n",
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_600.mp4'\n",
        "filename_out = 'video_frame_difference_full.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=1)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "filename_out = 'video_frame_difference_full_2.mp4'\n",
        "video_processor(filename_in, filename_out, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=0)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "kLCRvgrzPxmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 5\n",
        "Genere un video en un patio o en un hall de edificio donde en un principio se vea vacío y luego aparezca una persona. Mediante los métodos de motion detection (sin usar deep learning) logre una detección de la persona cuando entra al cuadro suponiendo la utilidad para una cámara de seguridad.\n",
        "Luego sobre el mismo video aplique los algoritmos de flujo denso y disperso que se mostraron en clase.\n",
        "Escriba una reflexión sobre los resultados en el formato md dentro del Jupyter Notebook.\n",
        "\n",
        "En este caso, el video que grabé lo subí a youtube para poder acceder a él más fácilmente ya que no podía subirlo local o a GitHub porque excedía en el tamaño."
      ],
      "metadata": {
        "id": "5HUhsFEWLazD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytube\n",
        "!apt update && apt install -y handbrake"
      ],
      "metadata": {
        "id": "TYeyNHD1LuOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# URL del video de YouTube\n",
        "url = 'https://www.youtube.com/shorts/9KOunnWc2Tg'\n",
        "\n",
        "# Crear un objeto YouTube\n",
        "yt = YouTube(url)\n",
        "\n",
        "# Seleccionar el stream con resolución de 720p\n",
        "video = yt.streams.filter(progressive=True, file_extension='mp4', res=\"720p\").first()\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "output_filename = 'video_ejer_5.mp4'\n",
        "\n",
        "# Descargar el video con el nombre especificado\n",
        "video.download(filename=output_filename)\n",
        "\n",
        "print(\"Descarga completada.\")"
      ],
      "metadata": {
        "id": "8DS3R30ML9Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto convierte el video a un formato más liviano de trabajo\n",
        "!ffmpeg -y -i video_ejer_5.mp4 -vf \"scale=600:-1\" -an -t 30 video_ejer_5_600.mp4"
      ],
      "metadata": {
        "id": "lRlMUeu8QS-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargo el video en menor resolución/calidad"
      ],
      "metadata": {
        "id": "OMRfNHgk3pzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # Descargar el archivo de video\n",
        "# files.download('/content/video_ejer_5_600.mp4')"
      ],
      "metadata": {
        "id": "65YxmsF13wwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba del video"
      ],
      "metadata": {
        "id": "KacEKKswNI4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapy\n",
        "import mediapy as media"
      ],
      "metadata": {
        "id": "bVh-zmVDSOJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'video_ejer_5_600.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "0o3A7Zn4MN5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reutilizo funciones anteriores para realizar el procesamiento del video\n",
        "\n"
      ],
      "metadata": {
        "id": "t5lvNSNlNkW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para oscurecer una imagen:\n",
        "def process_image(new_image, prev_image, **kwargs):\n",
        "    # Convertir la imagen a float32\n",
        "    new_image_float = new_image.astype(np.float32)\n",
        "\n",
        "    # Reducir el brillo de la imagen a la mitad\n",
        "    new_image_float *= 0.5\n",
        "\n",
        "    # Convertir la imagen de vuelta a uint8\n",
        "    new_image_uint8 = np.clip(new_image_float, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return new_image_uint8\n",
        "\n",
        "def draw_contours(frame, contours, color=(0, 255, 0), thickness=2):\n",
        "    # Comprobar si la imagen es en escala de grises (1 canal)\n",
        "    if len(frame.shape) == 2 or frame.shape[2] == 1:\n",
        "        # Convertir la imagen de escala de grises a color (3 canales)\n",
        "        result_image = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "    else:\n",
        "        # Si ya es una imagen de color, simplemente hacer una copia\n",
        "        result_image = frame.copy()\n",
        "\n",
        "    # Dibujar cada contorno en la imagen\n",
        "    for contour in contours:\n",
        "        # Obtener el rectángulo delimitador para cada contorno\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        # Dibujar el rectángulo\n",
        "        cv2.rectangle(result_image, (x, y), (x + w, y + h), color, thickness)\n",
        "\n",
        "    return result_image\n",
        "\n",
        "# Función para procesar un video:\n",
        "def video_processor(filename_in, filename_out, process_func, max_time=10, **kwargs):\n",
        "    # Abrir el video de entrada para lectura\n",
        "    with media.VideoReader(filename_in) as r:\n",
        "        # Crear un archivo de video de salida\n",
        "        with media.VideoWriter(filename_out, shape=r.shape, fps=r.fps, bps=r.bps) as w:\n",
        "            count = 0  # Inicializar contador de fotogramas\n",
        "            prev_image = None  # Inicializar la imagen previa\n",
        "\n",
        "            # Iterar sobre cada imagen (fotograma) del video\n",
        "            for image in r:\n",
        "                new_image = media.to_uint8(image)  # Convertir la imagen a formato flotante\n",
        "\n",
        "                # Comprobar si es la primera imagen\n",
        "                if prev_image is None:\n",
        "                    prev_image = new_image.copy()\n",
        "\n",
        "                # Procesar la imagen utilizando la función dada\n",
        "                processed_image = process_func(new_image, prev_image, **kwargs)\n",
        "\n",
        "                # Añadir la imagen procesada al video de salida\n",
        "                w.add_image(processed_image)\n",
        "\n",
        "                # Actualizar la imagen previa\n",
        "                prev_image = new_image.copy()\n",
        "\n",
        "                # Incrementar el contador de fotogramas\n",
        "                count += 1\n",
        "\n",
        "                # Detener el proceso si se alcanza el tiempo máximo\n",
        "                if count >= max_time * r.fps:\n",
        "                    break"
      ],
      "metadata": {
        "id": "ADVKl_WW2rp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_ejer_5_600.mp4'\n",
        "filename_out = 'video_ejer_5_dark.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_image, 10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "P32uM4aJNcX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motion detection"
      ],
      "metadata": {
        "id": "DA9W85pOiimH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame_difference_full(new_image, prev_image, min_area_threshold=13000, **kwargs):\n",
        "    # Convertir las imágenes a escala de grises\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
        "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
        "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
        "\n",
        "    # Normalizar la imagen de diferencia\n",
        "    norm_diff = cv2.normalize(frame_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # --- FILTRO BLUR (homogeneizar fondo)\n",
        "    img_blur = cv2.medianBlur(norm_diff, 5, 2)\n",
        "\n",
        "    # Umbralizar la imagen para resaltar las diferencias\n",
        "    _, thresh = cv2.threshold(img_blur, 60, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # # # Apertura para eliminar pequeñas porciones de píxeles\n",
        "    # opening_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
        "    # open = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, opening_kernel)\n",
        "\n",
        "    # Dilatar la imagen umbralizada para mejorar la detección de contornos\n",
        "    # kernel = np.ones((5,5),np.uint8)\n",
        "    opening_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (23,15))\n",
        "    dilated = cv2.dilate(thresh, opening_kernel, iterations = 3)\n",
        "\n",
        "    # Convertir la imagen dilatada a formato adecuado para findContours\n",
        "    dilated = dilated.astype(np.uint8)\n",
        "\n",
        "    # Encontrar contornos en la imagen dilatada\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filtrar los contornos por área mínima\n",
        "    filtered_contours = []\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) >= min_area_threshold:\n",
        "            filtered_contours.append(contour)\n",
        "\n",
        "    # Dibujar cuadros delimitadores alrededor de los contornos\n",
        "    if kwargs.get('draw_mode', 0) == 0:\n",
        "      result_image = draw_contours(new_image, filtered_contours)\n",
        "    elif kwargs.get('draw_mode', 0) == 1:\n",
        "      result_image = draw_contours(thresh, filtered_contours)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "pfT3Y6FFRDOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_ejer_5_600.mp4'\n",
        "filename_out = 'video_frame_difference_full.mp4'\n",
        "\n",
        "# # Llamar a la función para procesar el video\n",
        "# video_processor(filename_in, filename_out, process_frame_difference_full,\n",
        "#                 max_time=10, draw_mode=1)\n",
        "\n",
        "# # Mostrar el video resultante\n",
        "# media.show_video(media.read_video(filename_out), fps=30)\n",
        "\n",
        "#Llamar a la función para procesar el video\n",
        "filename_out = 'video_frame_difference_full_2.mp4'\n",
        "video_processor(filename_in, filename_out, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=0)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "QNgKzSuqPFdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse flow\n"
      ],
      "metadata": {
        "id": "PUISKsCHffot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy\n",
        "\n",
        "def process_sparse_optical_flow(new_image, prev_image):\n",
        "    # Preparamos las imagenes de trabajo\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "    prev_gray_image = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_blur = cv2.medianBlur(new_gray, 5, 2)\n",
        "\n",
        "    # Verificar si ya se han detectado las características de Shi-Tomasi\n",
        "    if not hasattr(process_sparse_optical_flow, \"shi_tomasi_done\"):\n",
        "        # Definir parámetros para la detección de esquinas de Shi-Tomasi\n",
        "        feature_params = dict(maxCorners=300, qualityLevel=0.2, minDistance=2, blockSize=7)\n",
        "        # Detectar puntos característicos en la imagen\n",
        "        process_sparse_optical_flow.prev_points = cv2.goodFeaturesToTrack(img_blur, mask=None, **feature_params)\n",
        "        # Crear una máscara para dibujar el flujo óptico\n",
        "        process_sparse_optical_flow.mask = np.zeros_like(new_image)\n",
        "        # Marcar que se ha completado la detección de Shi-Tomasi\n",
        "        process_sparse_optical_flow.shi_tomasi_done = True\n",
        "\n",
        "    # Continuar si se ha completado la detección de Shi-Tomasi\n",
        "    if process_sparse_optical_flow.shi_tomasi_done:\n",
        "        prev_points = process_sparse_optical_flow.prev_points\n",
        "        mask = process_sparse_optical_flow.mask\n",
        "\n",
        "    # Parámetros para el flujo óptico de Lucas-Kanade\n",
        "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
        "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "    # Calcular el flujo óptico de Lucas-Kanade\n",
        "    new_points, status, error = cv2.calcOpticalFlowPyrLK(prev_gray_image, img_blur, prev_points, None, **lk_params)\n",
        "    # Filtrar puntos buenos\n",
        "    good_old = prev_points[status == 1]\n",
        "    good_new = new_points[status == 1]\n",
        "    color = (0, 255, 0)  # Color para el dibujo\n",
        "    # Dibujar el movimiento (flujo óptico)\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.astype(int).ravel()\n",
        "        c, d = old.astype(int).ravel()\n",
        "        mask = cv2.line(mask, (a, b), (c, d), color, 2)\n",
        "        new_image = cv2.circle(new_image, (a, b), 3, color, -1)\n",
        "\n",
        "    # Combinar la imagen actual con las líneas de flujo óptico dibujadas\n",
        "    output = cv2.add(new_image, mask)\n",
        "    # Actualizar puntos para el siguiente cuadro\n",
        "    process_sparse_optical_flow.prev_points = good_new.reshape(-1, 1, 2)\n",
        "    return output\n",
        "\n",
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_ejer_5_600.mp4'\n",
        "filename_out = 'video_ejer_5_600_sparse_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_sparse_optical_flow,\n",
        "                max_time=10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "P8i3E33ffeQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense flow"
      ],
      "metadata": {
        "id": "H1VaJ90aiTlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para procesar el flujo óptico denso\n",
        "def process_dense_optical_flow(new_image, prev_image):\n",
        "    # Convierte la nueva imagen a escala de grises\n",
        "    gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if not hasattr(process_dense_optical_flow, \"init_done\"):\n",
        "        process_dense_optical_flow.prev_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "        process_dense_optical_flow.mask = np.zeros_like(new_image)\n",
        "        process_dense_optical_flow.mask[..., 1] = 255\n",
        "        process_dense_optical_flow.init_done = True\n",
        "\n",
        "    if process_dense_optical_flow.init_done:\n",
        "        prev_gray = process_dense_optical_flow.prev_gray\n",
        "        mask = process_dense_optical_flow.mask\n",
        "\n",
        "    # Calcula el flujo óptico\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    # Computa magnitud y ángulo de los vectores 2D\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    # Establece el tono de la imagen según la dirección del flujo óptico\n",
        "    mask[..., 0] = angle * 180 / np.pi / 2\n",
        "    # Establece el valor de la imagen según la magnitud del flujo óptico\n",
        "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    # Convierte de HSV a RGB\n",
        "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
        "    # Actualiza la imagen previa a gris\n",
        "    process_dense_optical_flow.prev_gray = gray.copy()\n",
        "    return rgb\n",
        "\n",
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_ejer_5_600.mp4'\n",
        "filename_out = 'video_ejer_5_600_dense_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, process_dense_optical_flow,\n",
        "                max_time=20)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "m1AhLHDciU42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados obtenidos acorde a la **detección de movimiento** fueron bastante satisfactorios. Se pudo detectar y segmentar correctamente el momento en el que la persona aparece en escena, como así también se la detecta cuando se va moviendo por el cuadro.\n",
        "\n",
        "Se tuvieron una serie de inconvenientes con los que lidiar:\n",
        "\n",
        "En primer lugar, había un poco de viento cuando se grabó el video, por lo tanto las hojas de las plantas se movian un poco. Esto generaba ruido a la hora de procesar el video y en consecuencia, hacía que se detecte ese movimiento también y no era lo que se quería. Para resolverlo, agregue un filtro BLUR para homogeneizar el fondo y de esta manera se elimino gran parte de ese ruido. Además agregué también el parámetro *min_area_threshold* ya que aún se detectaba algo de ruido y de esta forma, el contorno se dibuja en objetos a partir de ese límite establecido (seteado en un valor bastante alto).\n",
        "\n",
        "Se probó también aplicando una apertura antes de la dilatación, pero esto hacía desaparecer gran parte de los píxeles de la figura de la persona y luego no se podía llegar a segmentar correctamente.\n",
        "\n",
        "Después de jugar bastante tiempo con los parámetros tanto del filtro blur, como del umbral y de la dilatación se llegó al resultado que se muestra.\n",
        "\n",
        "Otro problema que se tiene es que, en la detección de movimiento, se detecta también la sombra de la persona, esto puede ser una contra como también algo bueno, ya que si por algún motivo la persona nunca aparece en escena, la cámara de seguridad podría detectar de todas formas que hay alguien en ese lugar sólo por su sombra.\n",
        "\n",
        "---\n",
        "\n",
        "Para el caso del **Flujo disperso** también se aplicó un filtro blur para homogeneizar el fondo y se obtuvo un buen resultado donde se determina el flujo de movimiento de la persona en el video. Se observa que la detección de movimiento se da en partes puntuales de la figura, como en las manos, cabeza y pies.\n",
        "\n",
        "---\n",
        "\n",
        "En el caso del **Flujo denso** también se obtuvo un buen resultado al determinar el flujo de movimiento pero en este caso, se tardó bastante más tiempo en finalizar la ejecución del algoritmo, llegando a los 2 min. cuando en el caso anterior se completó en unos 25 seg. También se observa que se detecta el movimiento de la sombra en esta oportunidad."
      ],
      "metadata": {
        "id": "n8dPgd8J6HW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 6\n",
        "Explique cuál es diferencia entre localización de objetos y clasificación de imágenes. Muestre ejemplos de ello."
      ],
      "metadata": {
        "id": "xq7C-iOMWGRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diferencias entre clasificación de imágenes y localización de objetos:\n",
        "\n",
        "Los modelos de **clasificación de imágenes** son algoritmos diseñados para identificar y etiquetar o clasificar objetos o características dentro de imágenes, es decir, estos modelos toman una imagen como entrada y predicen la clase o categoría a la que pertenece esa imagen en su totalidad, lo hacen sin precisión, o sea que nos limita a saber la cantidad de instancias que hay, donde estan ubicados, etc. La salida en los modelos de clasificación de imágenes suele ser una sola etiqueta que representa la categoría dominante de la imagen de entrada.\n",
        "Por ejemplo, en un conjunto de imágenes de perros y gatos, la clasificación de imágenes determinaría si la imagen muestra un perro o un gato.\n",
        "\n",
        "\n",
        "En cambio, los modelos de **localización de objetos** combinan la clasificación de imágenes y la localización para identificar múltiples objetos en imágenes y determinar su posición exacta a través de un bounding box.\n",
        "La localización de objetos supera a la clasificación de imágenes en importancia práctica, ya que permite ubicar objetos en la imagen, para su posterior análisis, modificación o clasificación. Continuando con el ejemplo de una imagen con perros y gatos, la localización de objetos identificaría la presencia de cada animal y delimitaría un cuadro alrededor de ellos para indicar su ubicación y extensión.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bFfexAG9p3Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplos"
      ],
      "metadata": {
        "id": "LnXQnTW15WI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clasificación de imágenes:\n",
        "\n",
        " Para este ejemplo voy a utilizar una foto de mi perro."
      ],
      "metadata": {
        "id": "tRFrH9Rn8zny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestro la imagen\n",
        "img = cv2.imread('foto_img_classif.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rBa5ArmhORg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizo el modelo preentrenado de EfficientNet80 para luego usarlo para clasificar mi imagen."
      ],
      "metadata": {
        "id": "Yc6ZOtJ2PNP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el modelo preentrenado EfficientNetB0\n",
        "model = EfficientNetB0(weights='imagenet')\n",
        "\n",
        "def classify_image(img_path):\n",
        "    # Cargar y preparar la imagen\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((224, 224))  # Tamaño de entrada para EfficientNetB0\n",
        "\n",
        "    # Convertir la imagen a un array y procesarla\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Realizar la predicción con el modelo\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    # Decodificar y retornar las predicciones\n",
        "    return decode_predictions(predictions, top=3)[0]"
      ],
      "metadata": {
        "id": "-3AsYAz7Hjs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accedo a la imagen contenida en el repo\n",
        "img_path = 'foto_img_classif.jpg'\n",
        "predictions = classify_image(img_path)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Mostrar las predicciones\n",
        "for i, (imagenet_id, label, score) in enumerate(predictions):\n",
        "    print(f\"{i + 1}: {label} ({score:.2f})\")\n",
        "\n",
        "# img = cv2.imread('foto_img_classif.jpg')"
      ],
      "metadata": {
        "id": "_pnZgGxBHjdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo clasificó correctamente la imagen de mi perro como un Boxer."
      ],
      "metadata": {
        "id": "blA55ymaPest"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Localización de objetos:\n",
        "\n",
        "Para este caso voy a utilizar una foto de mi novia andando en bicicleta por la calle."
      ],
      "metadata": {
        "id": "LygL87CW88Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "# Librería mediapy para mostrar videos\n",
        "%pip install -q mediapy\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "WjU8SUZ4HK-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías a utilizar\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image\n",
        "\n",
        "# Load the pre-trained YOLOv8 model\n",
        "model = YOLO('yolov8n')  # This will automatically download the model weights\n",
        "\n",
        "# Perform inference\n",
        "source_img = cv2.imread('foto_img_detect.jpg')\n",
        "results = model(source_img)\n",
        "\n",
        "# Visualize the results on the frame\n",
        "annotated_frame = results[0].plot()\n",
        "\n",
        "# Guardamos el resultado en JPG\n",
        "cv2.imwrite('results.jpg', annotated_frame)\n",
        "\n",
        "# Mostramos el resultado en Colab\n",
        "Image('results.jpg')"
      ],
      "metadata": {
        "id": "320x0AeDTEpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La localización de objeto se realizó de manera satisfactoria, solamente veo que en el caso de la camioneta que esta medio tapada la detectó como si fueran dos autos distintos. Los demás objetos observo que se han detectado bien.\n",
        "\n",
        "Me resulta interesante que se haya localizado la mochila de forma correcta, eso me sorprendió."
      ],
      "metadata": {
        "id": "1-bQM8RNWBmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 8\n",
        "\n",
        "En lo posible, realizar videos donde una clase de objeto difiera en número con otros objetos (sino, utilizar videos bajados de internet). Por ejemplo, al aire libre, un perro que se mueva entre personas en un parque. Una persona en bicicleta entre muchos autos en una calle de la ciudad. Luego, utilizar los distintos algoritmos presentados en teoría para detectar solo la clase minoritaria con su respectiva etiqueta. Indicando en cada caso la herramienta utilizada y comparando los niveles de confianza de detección logrados.\n",
        "\n",
        "\n",
        "En este caso voy a utilizar un video de youtube de un bicimensajero pedaleando por las calles de NY."
      ],
      "metadata": {
        "id": "TcoFl0SkW1VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytube\n",
        "!apt update && apt install -y handbrake"
      ],
      "metadata": {
        "id": "8Fc0ZXZFWue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# URL del video de YouTube\n",
        "url = 'https://www.youtube.com/watch?v=Q3sJOToxseU'\n",
        "\n",
        "# Crear un objeto YouTube\n",
        "yt = YouTube(url)\n",
        "\n",
        "# Seleccionar el stream con resolución de 720p\n",
        "video = yt.streams.filter(progressive=True, file_extension='mp4', res=\"720p\").first()\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "output_filename = 'video_ejer_8.mp4'\n",
        "\n",
        "# Descargar el video con el nombre especificado\n",
        "video.download(filename=output_filename)\n",
        "\n",
        "print(\"Descarga completada.\")"
      ],
      "metadata": {
        "id": "96QXHCJXt2cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#Esto convierte el video a un formato más liviano de trabajo\n",
        "# con -ss 126 indico el recorte que deseo del video (desde el min 2:06)\n",
        "# ya que en ese momento del video se visualiza solo un ciclista por la calle (la clase única y minoritaria en este caso)\n",
        "\n",
        "!ffmpeg -y -i video_ejer_8.mp4 -ss 126 -vf \"scale=600:-1\" -an -t 15 video_ejer_8_600.mp4\n"
      ],
      "metadata": {
        "id": "x7Ra6Bzat7jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargo el video con el formato y resolución modificados"
      ],
      "metadata": {
        "id": "HRpZ9dqfDsKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # Descargar el archivo de video\n",
        "# files.download('/content/video_ejer_8_600.mp4')"
      ],
      "metadata": {
        "id": "jIoPhEkUDrM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapy\n",
        "import mediapy as media"
      ],
      "metadata": {
        "id": "rcJ-Vpm45hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizo el video en formato ya modificado"
      ],
      "metadata": {
        "id": "f9M1L9kmqQGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'video_ejer_8_600.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "4WVrSmNMt_Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencia sobre video aplicando modelo YOLO"
      ],
      "metadata": {
        "id": "V1yA2RnBq_pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics"
      ],
      "metadata": {
        "id": "oqKY8B68E1BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "M54D7G4zEw6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! [ ! -f video_ejer_8_600.mp4 ] &&  wget https://raw.githubusercontent.com/euglpz/computer_vision/main/unidad_1/video_ejer_8_600.mp4\n",
        "!yolo predict model=yolov8n.pt source='video_ejer_8_600.mp4' save=True project=video_ejer_8_yolov8"
      ],
      "metadata": {
        "id": "i1R8XmudBtnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos a MP4 para comprimir el video y optimizar el uso de memoria para la reproducción con mediapy\n",
        "%%capture\n",
        "!ffmpeg -i 'video_ejer_8_yolov8/predict/video_ejer_8_600.avi' -vcodec libx264 'video_ejer_8_yolov8/predict/video_ejer_8_600.mp4' -y"
      ],
      "metadata": {
        "id": "oFc_yO4_GFrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualización del modelo YOLO funcionando"
      ],
      "metadata": {
        "id": "wI0_CsP4qyp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q mediapy\n",
        "# import mediapy as media\n",
        "url = 'video_ejer_8_yolov8/predict/video_ejer_8_600.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "cSgjFTBQGWss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pueden notar las localizaciones y segmentaciones de los objetos que se detectan a partir del modelo YOLO, como así también las etiquetas de esos objetos detectados en todas sus instancias.\n",
        "\n",
        "En este caso me interesa detectar solamente el objeto de clase **'bicycle'**.\n",
        "\n",
        "Para ello utilizo nuevamente el modelo YOLO y también las funciones desarrolladas en teoría para la detección de objeto y procesamiento de video.\n",
        "\n",
        "A la hora de detectar el objeto, se le pasa como parámetro a la función de detección solamente la clase 'bicycle', para que de esta forma solo se realice la detección y segmentación de esta clase en el video."
      ],
      "metadata": {
        "id": "hW8PHXi-rLOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy as media  # Importar la biblioteca mediapy para manejo de medios\n",
        "import cv2  # Importar OpenCV para el procesamiento de imágenes y videos\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained YOLOv8 model\n",
        "model = YOLO('yolov8n')  # This will automatically download the model weights\n",
        "\n",
        "# Función para detectar objetos en una imagen:\n",
        "def detect_objects(new_image, prev_image, **kwargs):\n",
        "    # Convertir la imagen a float32\n",
        "    results = model(new_image)\n",
        "\n",
        "    # Clases de interés para graficar\n",
        "    classes = kwargs.get('classes', ['car','person','bicycle', 'bus'])\n",
        "\n",
        "    # Iteramos sobre los boung boxes obtenidos\n",
        "    for box in results[0].boxes:\n",
        "        # Extrayendo los datos del tensor\n",
        "        x1, y1, x2, y2, confidence, cls = box.data[0]\n",
        "\n",
        "        # Obteniendo el nombre de la clase\n",
        "        class_name = model.names[int(cls)]\n",
        "\n",
        "        # Parámetros opcionales del bounding box\n",
        "        color = kwargs.get('color', (0, 255, 0))\n",
        "        thickness = kwargs.get('thickness', 2)\n",
        "\n",
        "        print(f\"Clase: {class_name}, Box: ({x1:.2f}, {y1:.2f}, {x2:.2f}, {y2:.2f}), Confianza: {confidence:.2f}\")\n",
        "\n",
        "        if class_name in classes:\n",
        "            # Dibujar el rectángulo\n",
        "            cv2.rectangle(new_image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
        "\n",
        "            # Agregar el texto de la confianza\n",
        "            confidence_text = f\"{class_name}: {confidence:.2f}\"\n",
        "            cv2.putText(new_image, confidence_text, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "# Función para procesar un video:\n",
        "def video_processor(filename_in, filename_out, process_func, max_time=10, **kwargs):\n",
        "    # Abrir el video de entrada para lectura\n",
        "    with media.VideoReader(filename_in) as r:\n",
        "        # Crear un archivo de video de salida\n",
        "        with media.VideoWriter(filename_out, shape=r.shape, fps=r.fps, bps=r.bps) as w:\n",
        "            count = 0  # Inicializar contador de fotogramas\n",
        "            prev_image = None  # Inicializar la imagen previa\n",
        "\n",
        "            # Iterar sobre cada imagen (fotograma) del video\n",
        "            for image in r:\n",
        "                new_image = media.to_uint8(image)  # Convertir la imagen a formato flotante\n",
        "\n",
        "                # Comprobar si es la primera imagen\n",
        "                if prev_image is None:\n",
        "                    prev_image = new_image.copy()\n",
        "\n",
        "                # Procesar la imagen utilizando la función dada\n",
        "                processed_image = process_func(new_image, prev_image, **kwargs)\n",
        "\n",
        "                # Añadir la imagen procesada al video de salida\n",
        "                w.add_image(processed_image)\n",
        "\n",
        "                # Actualizar la imagen previa\n",
        "                prev_image = new_image.copy()\n",
        "\n",
        "                # Incrementar el contador de fotogramas\n",
        "                count += 1\n",
        "\n",
        "                # Detener el proceso si se alcanza el tiempo máximo\n",
        "                if count >= max_time * r.fps:\n",
        "                    break"
      ],
      "metadata": {
        "id": "0HQtL_KxGaDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "filename_in = 'video_ejer_8_600.mp4'\n",
        "filename_out = 'video_ejer_8_600_bboxes.mp4'\n",
        "parameters = dict(classes=['bicycle'])\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(filename_in, filename_out, detect_objects, 20, **parameters)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(filename_out), fps=30)"
      ],
      "metadata": {
        "id": "CNYLhax0moAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede visualizar que de esta forma se pudo localizar correctamente a la clase minoritaria (bicicleta) en el video, dejando de lado todos los demás objetos."
      ],
      "metadata": {
        "id": "ca5DHs3Ltv15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'MobileNetSSD_deploy.prototxt' https://raw.githubusercontent.com/TheNsBhasin/DNN_Object_Detection/master/MobileNetSSD_deploy.prototxt.txt\n",
        "!wget -O 'MobileNetSSD_deploy.caffemodel' https://github.com/TheNsBhasin/DNN_Object_Detection/blob/master/MobileNetSSD_deploy.caffemodel?raw=true"
      ],
      "metadata": {
        "id": "JM6XCszJwdKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make sure the video file is in the same directory as your code\n",
        "filename = 'video_ejer_8_600.mp4'\n",
        "file_size = (1920,1080) # Assumes 1920x1080 mp4\n",
        "\n",
        "# We want to save the output to a video file\n",
        "output_filename = 'mobile_net.mp4'\n",
        "output_frames_per_second = 20.0\n",
        "\n",
        "RESIZED_DIMENSIONS = (300, 300) # Dimensions that SSD was trained on.\n",
        "IMG_NORM_RATIO = 0.007843 # In grayscale a pixel can range between 0 and 255\n",
        "\n",
        "# Load the pre-trained neural network\n",
        "neural_network = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt',\n",
        "        'MobileNetSSD_deploy.caffemodel')\n",
        "\n",
        "# List of categories and classes\n",
        "categories = { 0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird',\n",
        "               4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat',\n",
        "               9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog',\n",
        "              13: 'horse', 14: 'motorbike', 15: 'person',\n",
        "              16: 'pottedplant', 17: 'sheep', 18: 'sofa',\n",
        "              19: 'train', 20: 'tvmonitor'}\n",
        "\n",
        "classes =  [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "            \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "           \"diningtable\",  \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "           \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "# Create the bounding boxes\n",
        "bbox_colors = np.random.uniform(255, 0, size=(len(categories), 3))\n",
        "\n",
        "def main():\n",
        "\n",
        "  # Load a video\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "\n",
        "  # Create a VideoWriter object so we can save the video output\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  result = cv2.VideoWriter(output_filename,\n",
        "                           fourcc,\n",
        "                           output_frames_per_second,\n",
        "                           file_size)\n",
        "\n",
        "  # Process the video\n",
        "  while cap.isOpened():\n",
        "\n",
        "    # Capture one frame at a time\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    # Do we have a video frame? If true, proceed.\n",
        "    if success:\n",
        "\n",
        "      # Capture the frame's height and width\n",
        "      (h, w) = frame.shape[:2]\n",
        "\n",
        "      # Create a blob. A blob is a group of connected pixels in a binary\n",
        "      # frame that share some common property (e.g. grayscale value)\n",
        "      # Preprocess the frame to prepare it for deep learning classification\n",
        "      frame_blob = cv2.dnn.blobFromImage(cv2.resize(frame, RESIZED_DIMENSIONS),\n",
        "                     IMG_NORM_RATIO, RESIZED_DIMENSIONS, 127.5)\n",
        "\n",
        "      # Set the input for the neural network\n",
        "      neural_network.setInput(frame_blob)\n",
        "\n",
        "      # Predict the objects in the image\n",
        "      neural_network_output = neural_network.forward()\n",
        "\n",
        "      # Put the bounding boxes around the detected objects\n",
        "      for i in np.arange(0, neural_network_output.shape[2]):\n",
        "\n",
        "        confidence = neural_network_output[0, 0, i, 2]\n",
        "\n",
        "        # Confidence must be at least 30%\n",
        "        if confidence > 0.20:\n",
        "\n",
        "          idx = int(neural_network_output[0, 0, i, 1])\n",
        "\n",
        "          bounding_box = neural_network_output[0, 0, i, 3:7] * np.array(\n",
        "            [w, h, w, h])\n",
        "\n",
        "          (startX, startY, endX, endY) = bounding_box.astype(\"int\")\n",
        "\n",
        "          label = \"{}: {:.2f}%\".format(classes[idx], confidence * 100)\n",
        "\n",
        "          cv2.rectangle(frame, (startX, startY), (\n",
        "            endX, endY), bbox_colors[idx], 2)\n",
        "\n",
        "          y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "\n",
        "          cv2.putText(frame, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.5, bbox_colors[idx], 2)\n",
        "\n",
        "      # We now need to resize the frame so its dimensions\n",
        "      # are equivalent to the dimensions of the original frame\n",
        "      frame = cv2.resize(frame, file_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # Write the frame to the output video file\n",
        "      result.write(frame)\n",
        "\n",
        "    # No more video frames left\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  # Stop when the video is finished\n",
        "  cap.release()\n",
        "\n",
        "  # Release the video recording\n",
        "  result.release()\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "8CvAMVh6mx47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probando el método MobileNet SSD"
      ],
      "metadata": {
        "id": "3BHAR9b57L_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "modelo = 'MobileNetSSD_deploy.caffemodel'\n",
        "configuracion = 'MobileNetSSD_deploy.prototxt'\n",
        "clases = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
        "          \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "          \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
        "          \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "# Cargar la red neuronal\n",
        "net = cv2.dnn.readNetFromCaffe(configuracion, modelo)\n",
        "\n",
        "# Capturar el video desde una fuente (puedes cambiar el nombre del archivo o usar 0 para la cámara web)\n",
        "video = cv2.VideoCapture('video_ejer_8_600.mp4')\n",
        "\n",
        "# Obtener información del video\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "ancho = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "alto = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Definir el codec y crear el objeto VideoWriter\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_salida = cv2.VideoWriter('mobile_net.mp4', fourcc, fps, (ancho, alto))\n",
        "\n",
        "bbox_colors = np.random.uniform(255, 0, size=(len(categories), 3))\n",
        "\n",
        "# Iterar sobre cada fotograma del video\n",
        "while True:\n",
        "    # Leer el siguiente fotograma del video\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preparar el fotograma para la detección\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "\n",
        "    # Pasar el blob a través de la red y obtener las detecciones\n",
        "    net.setInput(blob)\n",
        "    detecciones = net.forward()\n",
        "\n",
        "    # Iterar sobre las detecciones\n",
        "    for i in np.arange(0, detecciones.shape[2]):\n",
        "        confianza = detecciones[0, 0, i, 2]\n",
        "\n",
        "        # Filtrar detecciones débiles\n",
        "        if confianza > 0.2:\n",
        "            idx = int(detecciones[0, 0, i, 1])\n",
        "            box = detecciones[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # Dibujar la predicción en el fotograma\n",
        "            etiqueta = \"{}: {:.2f}%\".format(clases[idx], confianza * 100)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), bbox_colors[idx], 2)\n",
        "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "            cv2.putText(frame, etiqueta, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, bbox_colors[idx], 2)\n",
        "\n",
        "    # Escribir el fotograma con detecciones en el video de salida\n",
        "    video_salida.write(frame)\n",
        "\n",
        "# Liberar los recursos y cerrar las ventanas\n",
        "video.release()\n",
        "video_salida.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "wcdBc_Um_kAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos el video generado"
      ],
      "metadata": {
        "id": "DJFX2kUV_6W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media.show_video(media.read_video(\"/content/mobile_net.mp4\"), fps=30)"
      ],
      "metadata": {
        "id": "jH9x3Yj5_vTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probando MobileNetSSD solo para la clase 'bicycle'"
      ],
      "metadata": {
        "id": "UmJr5q1w_-Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "modelo = 'MobileNetSSD_deploy.caffemodel'\n",
        "configuracion = 'MobileNetSSD_deploy.prototxt'\n",
        "clases = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
        "          \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "          \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
        "          \"sofa\", \"train\", \"tvmonitor\"]\n",
        "clase_interes = \"bicycle\"\n",
        "\n",
        "# Cargar la red neuronal\n",
        "net = cv2.dnn.readNetFromCaffe(configuracion, modelo)\n",
        "\n",
        "# Capturar el video desde una fuente (puedes cambiar el nombre del archivo o usar 0 para la cámara web)\n",
        "video = cv2.VideoCapture('video_ejer_8_600.mp4')\n",
        "\n",
        "# Obtener información del video\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "ancho = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "alto = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Definir el codec y crear el objeto VideoWriter\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_salida = cv2.VideoWriter('mobile_net_bicycle.mp4', fourcc, fps, (ancho, alto))\n",
        "\n",
        "bbox_colors = np.random.uniform(255, 0, size=(len(categories), 3))\n",
        "\n",
        "# Iterar sobre cada fotograma del video\n",
        "while True:\n",
        "    # Leer el siguiente fotograma del video\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preparar el fotograma para la detección\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "\n",
        "    # Pasar el blob a través de la red y obtener las detecciones\n",
        "    net.setInput(blob)\n",
        "    detecciones = net.forward()\n",
        "\n",
        "    # Iterar sobre las detecciones\n",
        "    for i in np.arange(0, detecciones.shape[2]):\n",
        "        confianza = detecciones[0, 0, i, 2]\n",
        "\n",
        "        # Filtrar detecciones débiles\n",
        "        if confianza > 0.2:\n",
        "            idx = int(detecciones[0, 0, i, 1])\n",
        "            if clase_interes in clases[idx]:\n",
        "              box = detecciones[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "              (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "              # Dibujar la predicción en el fotograma\n",
        "              etiqueta = \"{}: {:.2f}%\".format(clases[idx], confianza * 100)\n",
        "              cv2.rectangle(frame, (startX, startY), (endX, endY), bbox_colors[idx], 2)\n",
        "              y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "              cv2.putText(frame, etiqueta, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, bbox_colors[idx], 2)\n",
        "\n",
        "    # Escribir el fotograma con detecciones en el video de salida\n",
        "    video_salida.write(frame)\n",
        "\n",
        "# Liberar los recursos y cerrar las ventanas\n",
        "video.release()\n",
        "video_salida.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "lJFZ798847bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media.show_video(media.read_video(\"/content/mobile_net_bicycle.mp4\"), fps=30)"
      ],
      "metadata": {
        "id": "vaR9vMoUxAvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando el modelo de CNN MobileNetSSD vemos que solamente al comienzo del video puede localizar a la bicicleta, esto puede ocurrir porque quizás para el entrenamiento del modelo no se le pasaron muchas imágenes de bicicletas desde distintos ángulos, por ahí este modelo no es tan robusto para esa clase de objeto, como si lo es el modelo YOLO."
      ],
      "metadata": {
        "id": "2kV1vZ3Y-_UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 9\n",
        "\n",
        "Tomar fotografías donde coexistan varios objetos en posiciones solapadas y no, en contextos de diferente complejidad. Luego, aplicar los algoritmos de segmentación propuestos y verificar los resultados de cada uno. Comentar qué diferencias observa.\n",
        "\n",
        "Para este ejercicio voy a utilizar imágenes tomadas en la calle en distintas situaciones para ver como se comportan los distintos modelos."
      ],
      "metadata": {
        "id": "pKA8nzqszqIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instance Segmentation con Mask R-CNN (OpenCV)"
      ],
      "metadata": {
        "id": "wkKbdm_pOEw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "!tar zxvf mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "!wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/mscoco_labels.names -O mscoco_labels.names\n",
        "!wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/colors.txt -O colors.txt\n",
        "!wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/mask_rcnn_inception_v2_coco_2018_01_28.pbtxt -O mask_rcnn_inception_v2_coco_2018_01_28.pbtxt"
      ],
      "metadata": {
        "id": "2fkxNLkp8ueD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import time\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Umbral de confianza para las detecciones\n",
        "confThreshold = 0.5\n",
        "# Umbral para las máscaras de segmentación\n",
        "maskThreshold = 0.3\n",
        "\n",
        "# Cargar los nombres de las clases de MSCOCO\n",
        "classesFile = \"mscoco_labels.names\"\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "    classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Cargar los colores para visualizar las detecciones\n",
        "colorsFile = \"colors.txt\"\n",
        "colors = []\n",
        "with open(colorsFile, 'rt') as f:\n",
        "    colorsStr = f.read().rstrip('\\n').split('\\n')\n",
        "    for i in range(len(colorsStr)):\n",
        "        rgb = colorsStr[i].split(' ')\n",
        "        color = np.array([float(rgb[0]), float(rgb[1]), float(rgb[2])])\n",
        "        colors.append(color)\n",
        "\n",
        "# Archivos de configuración y pesos del modelo Mask R-CNN\n",
        "textGraph = \"./mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\"\n",
        "modelWeights = \"./mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\"\n",
        "\n",
        "# Cargar la red\n",
        "net = cv.dnn.readNetFromTensorflow(modelWeights, textGraph)\n",
        "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "# Cargar la imagen y obtener sus dimensiones\n",
        "image_1 = cv.imread('mza1.jpg')\n",
        "(H, W) = image_1.shape[:2]\n",
        "\n",
        "# Crear un blob de la imagen y realizar una pasada hacia adelante con Mask R-CNN\n",
        "blob = cv.dnn.blobFromImage(image_1, swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "(boxes, masks) = net.forward([\"detection_out_final\", \"detection_masks\"])\n",
        "end = time.time()\n",
        "\n",
        "# Información de tiempo y formas de las detecciones y máscaras\n",
        "print(\"[INFO] Mask R-CNN tomó {:.6f} segundos en procesar la imagen\".format(end - start))\n",
        "print(\"[INFO] Forma de boxes: {}\".format(boxes.shape))\n",
        "print(\"[INFO] Forma de masks: {}\".format(masks.shape))"
      ],
      "metadata": {
        "id": "DraQK2aDEI4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes\n",
        "image_1.shape[:2]"
      ],
      "metadata": {
        "id": "vcB88BjLE3bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una copia de la imagen original, para dibujar las máscaras\n",
        "output_image = image_1.copy()\n",
        "\n",
        "for i in range(0, boxes.shape[2]):\n",
        "    confidence = boxes[0, 0, i, 2]\n",
        "    if confidence > confThreshold:\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        box = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # Redimensiona la máscara para que se ajuste a la caja delimitadora y la convierte en uint8\n",
        "        mask = masks[i, classId]\n",
        "        mask = cv.resize(mask, (endX - startX, endY - startY))\n",
        "        mask = (mask > maskThreshold).astype(\"uint8\")\n",
        "\n",
        "        # Extrae la región de interés (ROI) usando las coordenadas de la caja delimitadora\n",
        "        roi = output_image[startY:endY, startX:endX]\n",
        "\n",
        "        # Crea una imagen de color sólido para la máscara\n",
        "        color = random.choice(colors)\n",
        "        color_mask = np.zeros_like(roi)\n",
        "        color_mask[mask == 1] = color\n",
        "\n",
        "        # Mezcla la imagen de color con la ROI utilizando la máscara\n",
        "        roi = cv.addWeighted(roi, 1, color_mask, 0.4, 0)\n",
        "\n",
        "        # Vuelve a colocar la ROI en la imagen original\n",
        "        output_image[startY:endY, startX:endX] = roi\n",
        "\n",
        "        # Dibuja la caja delimitadora\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "        # Consigue el nombre de la clase y el color correspondiente\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        className = classes[classId]\n",
        "\n",
        "        # Dibuja el rectángulo de la caja delimitadora y la etiqueta de la clase\n",
        "        label = f\"{className}: {confidence:.2f}\"\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "        cv.putText(output_image, label, (startX, startY - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "# Configurar los subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Mostrar la imagen original\n",
        "axes[0].imshow(cv.cvtColor(image_1, cv.COLOR_BGR2RGB))\n",
        "axes[0].set_title('Imagen Original')\n",
        "axes[0].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar la imagen segmentada\n",
        "axes[1].imshow(cv.cvtColor(output_image, cv.COLOR_BGR2RGB))\n",
        "axes[1].set_title('Imagen Segmentada')\n",
        "axes[1].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar ambas imágenes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wZqlE9jIEXSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la imagen y obtener sus dimensiones\n",
        "image_2 = cv.imread('mza2.jpg')\n",
        "(H2, W2) = image_2.shape[:2]\n",
        "\n",
        "# Crear un blob de la imagen y realizar una pasada hacia adelante con Mask R-CNN\n",
        "blob = cv.dnn.blobFromImage(image_2, swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "(boxes, masks) = net.forward([\"detection_out_final\", \"detection_masks\"])\n",
        "end = time.time()\n",
        "\n",
        "# Información de tiempo y formas de las detecciones y máscaras\n",
        "print(\"[INFO] Mask R-CNN tomó {:.6f} segundos en procesar la imagen\".format(end - start))\n",
        "print(\"[INFO] Forma de boxes: {}\".format(boxes.shape))\n",
        "print(\"[INFO] Forma de masks: {}\".format(masks.shape))"
      ],
      "metadata": {
        "id": "CSASe8xtNrCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una copia de la imagen original, para dibujar las máscaras\n",
        "output_image = image_2.copy()\n",
        "\n",
        "for i in range(0, boxes.shape[2]):\n",
        "    confidence = boxes[0, 0, i, 2]\n",
        "    if confidence > confThreshold:\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        box = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # Redimensiona la máscara para que se ajuste a la caja delimitadora y la convierte en uint8\n",
        "        mask = masks[i, classId]\n",
        "        mask = cv.resize(mask, (endX - startX, endY - startY))\n",
        "        mask = (mask > maskThreshold).astype(\"uint8\")\n",
        "\n",
        "        # Extrae la región de interés (ROI) usando las coordenadas de la caja delimitadora\n",
        "        roi = output_image[startY:endY, startX:endX]\n",
        "\n",
        "        # Crea una imagen de color sólido para la máscara\n",
        "        color = random.choice(colors)\n",
        "        color_mask = np.zeros_like(roi)\n",
        "        color_mask[mask == 1] = color\n",
        "\n",
        "        # Mezcla la imagen de color con la ROI utilizando la máscara\n",
        "        roi = cv.addWeighted(roi, 1, color_mask, 0.4, 0)\n",
        "\n",
        "        # Vuelve a colocar la ROI en la imagen original\n",
        "        output_image[startY:endY, startX:endX] = roi\n",
        "\n",
        "        # Dibuja la caja delimitadora\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "        # Consigue el nombre de la clase y el color correspondiente\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        className = classes[classId]\n",
        "\n",
        "        # Dibuja el rectángulo de la caja delimitadora y la etiqueta de la clase\n",
        "        label = f\"{className}: {confidence:.2f}\"\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "        cv.putText(output_image, label, (startX, startY - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "# Configurar los subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Mostrar la imagen original\n",
        "axes[0].imshow(cv.cvtColor(image_2, cv.COLOR_BGR2RGB))\n",
        "axes[0].set_title('Imagen Original')\n",
        "axes[0].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar la imagen segmentada\n",
        "axes[1].imshow(cv.cvtColor(output_image, cv.COLOR_BGR2RGB))\n",
        "axes[1].set_title('Imagen Segmentada')\n",
        "axes[1].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar ambas imágenes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eX5oWRqtNsFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_3 = cv.imread('sj1.jpg')\n",
        "(H2, W2) = image_3.shape[:2]\n",
        "\n",
        "# Crear un blob de la imagen y realizar una pasada hacia adelante con Mask R-CNN\n",
        "blob = cv.dnn.blobFromImage(image_2, swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "(boxes, masks) = net.forward([\"detection_out_final\", \"detection_masks\"])\n",
        "end = time.time()\n",
        "\n",
        "# Información de tiempo y formas de las detecciones y máscaras\n",
        "print(\"[INFO] Mask R-CNN tomó {:.6f} segundos en procesar la imagen\".format(end - start))\n",
        "print(\"[INFO] Forma de boxes: {}\".format(boxes.shape))\n",
        "print(\"[INFO] Forma de masks: {}\".format(masks.shape))"
      ],
      "metadata": {
        "id": "HtXJP34akUkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_image = image_3.copy()\n",
        "\n",
        "for i in range(0, boxes.shape[2]):\n",
        "    confidence = boxes[0, 0, i, 2]\n",
        "    if confidence > confThreshold:\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        box = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # Redimensiona la máscara para que se ajuste a la caja delimitadora y la convierte en uint8\n",
        "        mask = masks[i, classId]\n",
        "        mask = cv.resize(mask, (endX - startX, endY - startY))\n",
        "        mask = (mask > maskThreshold).astype(\"uint8\")\n",
        "\n",
        "        # Extrae la región de interés (ROI) usando las coordenadas de la caja delimitadora\n",
        "        roi = output_image[startY:endY, startX:endX]\n",
        "\n",
        "        # Crea una imagen de color sólido para la máscara\n",
        "        color = random.choice(colors)\n",
        "        color_mask = np.zeros_like(roi)\n",
        "        color_mask[mask == 1] = color\n",
        "\n",
        "        # Mezcla la imagen de color con la ROI utilizando la máscara\n",
        "        roi = cv.addWeighted(roi, 1, color_mask, 0.4, 0)\n",
        "\n",
        "        # Vuelve a colocar la ROI en la imagen original\n",
        "        output_image[startY:endY, startX:endX] = roi\n",
        "\n",
        "        # Dibuja la caja delimitadora\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "        # Consigue el nombre de la clase y el color correspondiente\n",
        "        classId = int(boxes[0, 0, i, 1])\n",
        "        className = classes[classId]\n",
        "\n",
        "        # Dibuja el rectángulo de la caja delimitadora y la etiqueta de la clase\n",
        "        label = f\"{className}: {confidence:.2f}\"\n",
        "        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)\n",
        "        cv.putText(output_image, label, (startX, startY - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "# Configurar los subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Mostrar la imagen original\n",
        "axes[0].imshow(cv.cvtColor(image_3, cv.COLOR_BGR2RGB))\n",
        "axes[0].set_title('Imagen Original')\n",
        "axes[0].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar la imagen segmentada\n",
        "axes[1].imshow(cv.cvtColor(output_image, cv.COLOR_BGR2RGB))\n",
        "axes[1].set_title('Imagen Segmentada')\n",
        "axes[1].axis('off')  # Ocultar los ejes\n",
        "\n",
        "# Mostrar ambas imágenes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4eSFH7-ClkSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Segmentation con modelo PointRend Resnet50 (PixelLib)"
      ],
      "metadata": {
        "id": "gk7Kq3stTNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pixellib torchvision\n",
        "# Instance segmentation model\n",
        "!wget \"https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet50.pkl\" -O pointrend_resnet50.pkl"
      ],
      "metadata": {
        "id": "5sr7X6iZPY87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de la biblioteca PixelLib\n",
        "# !pip install pixellib\n",
        "\n",
        "# Importación de las bibliotecas necesarias\n",
        "import pixellib\n",
        "from pixellib.torchbackend.instance import instanceSegmentation\n",
        "from IPython.display import Image\n",
        "\n",
        "# Creación de una instancia del modelo de segmentación\n",
        "ins = instanceSegmentation()\n",
        "\n",
        "# Carga del modelo preentrenado\n",
        "ins.load_model(\"pointrend_resnet50.pkl\")\n",
        "\n",
        "# Segmentación de la imagen y guardado de la imagen segmentada con las cajas delimitadoras\n",
        "ins.segmentImage(\"mza1.jpg\", show_bboxes=True, output_image_name=\"mza1_segmented.jpg\")\n",
        "\n",
        "# Muestra la imagen original\n",
        "display(Image(filename=\"mza1.jpg\"))\n",
        "\n",
        "# Muestra la imagen segmentada\n",
        "display(Image(filename=\"mza1_segmented.jpg\"))\n"
      ],
      "metadata": {
        "id": "cAJLFSZuPdbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmentación de la imagen y guardado de la imagen segmentada con las cajas delimitadoras\n",
        "ins.segmentImage(\"mza2.jpg\", show_bboxes=True, output_image_name=\"mza2_segmented.jpg\")\n",
        "\n",
        "# Muestra la imagen original\n",
        "display(Image(filename=\"mza2.jpg\"))\n",
        "\n",
        "# Muestra la imagen segmentada\n",
        "display(Image(filename=\"mza2_segmented.jpg\"))\n"
      ],
      "metadata": {
        "id": "umuwgUf4P_qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins.segmentImage(\"sj1.jpg\", show_bboxes=True, output_image_name=\"sj1_segmented.jpg\")\n",
        "\n",
        "# Muestra la imagen original\n",
        "display(Image(filename=\"sj1.jpg\"))\n",
        "\n",
        "# Muestra la imagen segmentada\n",
        "display(Image(filename=\"sj1_segmented.jpg\"))"
      ],
      "metadata": {
        "id": "_KAHnQ9CnuS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Segmentation con modelo DeepLabV3 (Torchvision)"
      ],
      "metadata": {
        "id": "He9i9MV9TS9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torchvision\n",
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "LW3Xr8NmQHYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Carga y procesa la imagen\n",
        "input_image = Image.open(\"mza1.jpg\").convert(\"RGB\").convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # crea un mini-batch como espera el modelo\n",
        "\n",
        "# Mueve el input y el modelo a GPU para mayor velocidad si está disponible\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)\n",
        "\n",
        "# Crea una paleta de colores, seleccionando un color para cada clase\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# Genera la imagen de segmentación semántica\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "# Muestra la imagen original y la segmentada lado a lado\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, primer subplot\n",
        "plt.imshow(input_image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, segundo subplot\n",
        "plt.imshow(r)\n",
        "plt.title('Imagen Segmentada')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VRlc6eFKS_bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga y procesa la imagen\n",
        "input_image = Image.open(\"mza2.jpg\").convert(\"RGB\").convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # crea un mini-batch como espera el modelo\n",
        "\n",
        "# Mueve el input y el modelo a GPU para mayor velocidad si está disponible\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)\n",
        "\n",
        "# Crea una paleta de colores, seleccionando un color para cada clase\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# Genera la imagen de segmentación semántica\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "# Muestra la imagen original y la segmentada lado a lado\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, primer subplot\n",
        "plt.imshow(input_image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, segundo subplot\n",
        "plt.imshow(r)\n",
        "plt.title('Imagen Segmentada')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aQ0M0erPTxo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga y procesa la imagen\n",
        "input_image = Image.open(\"sj1.jpg\").convert(\"RGB\").convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # crea un mini-batch como espera el modelo\n",
        "\n",
        "# Mueve el input y el modelo a GPU para mayor velocidad si está disponible\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)\n",
        "\n",
        "# Crea una paleta de colores, seleccionando un color para cada clase\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# Genera la imagen de segmentación semántica\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "# Muestra la imagen original y la segmentada lado a lado\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, primer subplot\n",
        "plt.imshow(input_image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, segundo subplot\n",
        "plt.imshow(r)\n",
        "plt.title('Imagen Segmentada')\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7KBQ8AwwqhRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instance Segmentation con PSPNet (MMSegmentation)"
      ],
      "metadata": {
        "id": "_p1-k49fT9xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install -U openmim\n",
        "mim install mmengine\n",
        "mim install \"mmcv>=2.0.0\"\n",
        "pip install \"mmsegmentation>=1.0.0\" ftfy\n",
        "mim download mmsegmentation --config pspnet_r50-d8_4xb2-40k_cityscapes-512x1024 --dest .\n"
      ],
      "metadata": {
        "id": "qkU7W0y3Tyii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b main https://github.com/open-mmlab/mmsegmentation.git"
      ],
      "metadata": {
        "id": "lyv1UfYXt5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Instalación de librería MMSegmentation\n",
        "import sys\n",
        "!cd /content/mmsegmentation && {sys.executable} -m pip install -v -e ."
      ],
      "metadata": {
        "id": "pOJ05FJLtbNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/computer_vision/unidad_1/mmsegmentation')"
      ],
      "metadata": {
        "id": "dq3sCpHxuH2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"mmcv>=2.0.0rc4\""
      ],
      "metadata": {
        "id": "fJQtXt8av2hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
        "import mmcv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Configuración e inicialización del modelo\n",
        "config_file = '../pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py'\n",
        "checkpoint_file = '../pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
        "model = init_model(config_file, checkpoint_file, device='cuda:0')\n",
        "\n",
        "# Ruta de la imagen a segmentar\n",
        "img_path = '/content/mza1.jpg'\n",
        "\n",
        "# Inferencia del modelo\n",
        "result = inference_model(model, img_path)\n",
        "\n",
        "# display the segmentation result\n",
        "segmented_image = show_result_pyplot(model, img_path, result)\n",
        "\n",
        "# Visualización con matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(segmented_image)\n",
        "plt.axis('off')  # Oculta los ejes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0UZR25PWtrNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Segmentation con YoloV8 (Ultralytics)"
      ],
      "metadata": {
        "id": "mZYLbgtNT_ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "BQH73bOgUCVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# Cargar el modelo YOLOv8 con capacidad de segmentación\n",
        "model = YOLO(\"yolov8m-seg.pt\")\n",
        "\n",
        "# Realizar predicciones en la imagen\n",
        "results = model.predict('mza1.jpg')\n",
        "\n",
        "# Seleccionar el primer resultado (en caso de que haya varios)\n",
        "result = results[0]\n",
        "\n",
        "# Abrir la imagen original para dibujar sobre ella\n",
        "img = Image.open(\"mza1.jpg\")\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Para cada detección, dibujar el bounding box y mostrar la clase\n",
        "for box in results[0].boxes:\n",
        "    # Extracción de coordenadas del bounding box y clase\n",
        "    x1, y1, x2, y2, conf, cls = box.data[0]\n",
        "    label = result.names[int(cls)]  # Asumiendo que result.names contiene los nombres de las clases\n",
        "\n",
        "    # Dibujar el bounding box\n",
        "    draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=2)\n",
        "\n",
        "    # Mostrar la clase (y opcionalmente la confianza) en el bounding box\n",
        "    text = f\"{label} {conf:.2f}\"\n",
        "    draw.text((x1, y1), text, fill=(0, 255, 0))\n",
        "\n",
        "# Acceder a las máscaras del resultado\n",
        "masks = result.masks\n",
        "\n",
        "# Iterar a través de todas las máscaras encontradas\n",
        "for mask in masks:\n",
        "    # Convertir la máscara a un array numpy y obtener el polígono\n",
        "    mask_data = mask.data[0].numpy()\n",
        "    polygon = mask.xy[0]\n",
        "\n",
        "    # Dibujar el polígono correspondiente a cada máscara en la imagen original\n",
        "    polygon_list = [(p[0], p[1]) for p in polygon]\n",
        "    draw.polygon(polygon_list, outline=(255, 0, 0), width=3)\n",
        "\n",
        "# Mostrar la imagen con los polígonos dibujados\n",
        "display(img)"
      ],
      "metadata": {
        "id": "HlVGUgTq0JuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo YOLOv8 con capacidad de segmentación\n",
        "model = YOLO(\"yolov8m-seg.pt\")\n",
        "\n",
        "# Realizar predicciones en la imagen\n",
        "results = model.predict('mza2.jpg')\n",
        "\n",
        "# Seleccionar el primer resultado (en caso de que haya varios)\n",
        "result = results[0]\n",
        "\n",
        "# Abrir la imagen original para dibujar sobre ella\n",
        "img = Image.open(\"mza2.jpg\")\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Para cada detección, dibujar el bounding box y mostrar la clase\n",
        "for box in results[0].boxes:\n",
        "    # Extracción de coordenadas del bounding box y clase\n",
        "    x1, y1, x2, y2, conf, cls = box.data[0]\n",
        "    label = result.names[int(cls)]  # Asumiendo que result.names contiene los nombres de las clases\n",
        "\n",
        "    # Dibujar el bounding box\n",
        "    draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=2)\n",
        "\n",
        "    # Mostrar la clase (y opcionalmente la confianza) en el bounding box\n",
        "    text = f\"{label} {conf:.2f}\"\n",
        "    draw.text((x1, y1), text, fill=(0, 255, 0))\n",
        "\n",
        "# Acceder a las máscaras del resultado\n",
        "masks = result.masks\n",
        "\n",
        "# Iterar a través de todas las máscaras encontradas\n",
        "for mask in masks:\n",
        "    # Convertir la máscara a un array numpy y obtener el polígono\n",
        "    mask_data = mask.data[0].numpy()\n",
        "    polygon = mask.xy[0]\n",
        "\n",
        "    # Dibujar el polígono correspondiente a cada máscara en la imagen original\n",
        "    polygon_list = [(p[0], p[1]) for p in polygon]\n",
        "    draw.polygon(polygon_list, outline=(255, 0, 0), width=3)\n",
        "\n",
        "# Mostrar la imagen con los polígonos dibujados\n",
        "display(img)"
      ],
      "metadata": {
        "id": "9HXv0K9n0poC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo YOLOv8 con capacidad de segmentación\n",
        "model = YOLO(\"yolov8m-seg.pt\")\n",
        "\n",
        "# Realizar predicciones en la imagen\n",
        "results = model.predict('sj1.jpg')\n",
        "\n",
        "# Seleccionar el primer resultado (en caso de que haya varios)\n",
        "result = results[0]\n",
        "\n",
        "# Abrir la imagen original para dibujar sobre ella\n",
        "img = Image.open(\"sj1.jpg\")\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Para cada detección, dibujar el bounding box y mostrar la clase\n",
        "for box in results[0].boxes:\n",
        "    # Extracción de coordenadas del bounding box y clase\n",
        "    x1, y1, x2, y2, conf, cls = box.data[0]\n",
        "    label = result.names[int(cls)]  # Asumiendo que result.names contiene los nombres de las clases\n",
        "\n",
        "    # Dibujar el bounding box\n",
        "    draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=2)\n",
        "\n",
        "    # Mostrar la clase (y opcionalmente la confianza) en el bounding box\n",
        "    text = f\"{label} {conf:.2f}\"\n",
        "    draw.text((x1, y1), text, fill=(0, 255, 0))\n",
        "\n",
        "# Acceder a las máscaras del resultado\n",
        "masks = result.masks\n",
        "\n",
        "# Iterar a través de todas las máscaras encontradas\n",
        "for mask in masks:\n",
        "    # Convertir la máscara a un array numpy y obtener el polígono\n",
        "    mask_data = mask.data[0].numpy()\n",
        "    polygon = mask.xy[0]\n",
        "\n",
        "    # Dibujar el polígono correspondiente a cada máscara en la imagen original\n",
        "    polygon_list = [(p[0], p[1]) for p in polygon]\n",
        "    draw.polygon(polygon_list, outline=(255, 0, 0), width=3)\n",
        "\n",
        "# Mostrar la imagen con los polígonos dibujados\n",
        "display(img)"
      ],
      "metadata": {
        "id": "65Sv2nlordTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Panoptic segmentation con modelo DETR ResNet50 (Transformers)"
      ],
      "metadata": {
        "id": "eKehgoCXUCiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "N8iLqJmu1A6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "from transformers import DetrFeatureExtractor, DetrForSegmentation\n",
        "from transformers.models.detr.feature_extraction_detr import rgb_to_id\n",
        "\n",
        "# Cargar la imagen\n",
        "img_path = \"mza1.jpg\"\n",
        "image = Image.open(img_path)\n",
        "\n",
        "# Cargar el modelo y el extractor de características\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "\n",
        "# Preparar la imagen para el modelo\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Paso forward\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Postprocesamiento\n",
        "processed_sizes = torch.as_tensor(inputs[\"pixel_values\"].shape[-2:]).unsqueeze(0)\n",
        "result = feature_extractor.post_process_panoptic(outputs, processed_sizes, threshold=0.85)[0]\n",
        "\n",
        "# Convertir la segmentación a numpy\n",
        "panoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\n",
        "panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
        "panoptic_seg_id = rgb_to_id(panoptic_seg)\n",
        "\n",
        "# Preparar la paleta de colores\n",
        "palette = itertools.cycle(sns.color_palette())\n",
        "\n",
        "# Crear la imagen segmentada\n",
        "segmented_image = Image.fromarray(np.zeros_like(panoptic_seg, dtype=np.uint8))\n",
        "draw = ImageDraw.Draw(segmented_image)\n",
        "\n",
        "# Añadimos un mapeo manual de los IDs de las categorías a los nombres, basado en las clases comunes de COCO\n",
        "COCO_LABELS = {\n",
        "    1: 'persona', 2: 'bicicleta', 3: 'coche', 4: 'motocicleta', 5: 'avión',\n",
        "    6: 'autobús', 7: 'tren', 8: 'camión', 9: 'barco', 10: 'semáforo',\n",
        "    11: 'hidrante', 13: 'señal de stop', 14: 'parquímetro', 15: 'banco', 16: 'pájaro',\n",
        "    17: 'gato', 18: 'perro', 19: 'caballo', 20: 'oveja', 21: 'vaca',\n",
        "    22: 'elefante', 23: 'oso', 24: 'cebra', 25: 'jirafa', 27: 'mochila',\n",
        "    28: 'paraguas', 31: 'bolso de mano', 32: 'corbata', 33: 'maleta', 34: 'frisbee',\n",
        "    36: 'tabla de snowboard', 37: 'pelota deportiva', 38: 'cometa', 39: 'bate de béisbol',\n",
        "    40: 'guante de béisbol', 41: 'patineta', 42: 'tabla de surf', 43: 'raqueta de tenis',\n",
        "    44: 'botella', 46: 'plato de vino', 47: 'taza', 48: 'tenedor', 49: 'cuchillo',\n",
        "    50: 'cuchara', 51: 'tazón', 52: 'banana', 53: 'manzana', 54: 'sándwich',\n",
        "    55: 'naranja', 56: 'brócoli', 57: 'zanahoria', 58: 'perrito caliente', 59: 'pizza',\n",
        "    60: 'donut', 61: 'pastel', 62: 'silla', 63: 'sofá', 64: 'maceta', 65: 'cama',\n",
        "    67: 'mesa de comedor', 70: 'inodoro', 72: 'TV', 73: 'computadora portátil', 74: 'ratón',\n",
        "    75: 'control remoto', 76: 'teclado', 77: 'teléfono celular', 78: 'microondas',\n",
        "    79: 'horno', 80: 'tostadora', 81: 'fregadero', 82: 'refrigerador', 84: 'libro',\n",
        "    85: 'reloj', 86: 'florero', 87: 'tijeras', 88: 'oso de peluche', 89: 'secador de pelo',\n",
        "    90: 'cepillo de dientes',\n",
        "}\n",
        "\n",
        "# Ajusta el bucle para dibujar segmentos y etiquetas correctamente\n",
        "for segment_info in result[\"segments_info\"]:\n",
        "    class_id = segment_info[\"category_id\"]\n",
        "    class_name = COCO_LABELS.get(class_id, 'Desconocido')  # 'Desconocido' si el ID no está en el diccionario\n",
        "    id = segment_info[\"id\"]\n",
        "\n",
        "    # Generar la máscara para este segmento específico\n",
        "    mask = panoptic_seg_id == id\n",
        "    color = np.array(next(palette)) * 255  # Convertir el color a un array de numpy adecuado\n",
        "\n",
        "    # Convertir la máscara a una imagen de PIL para usarla como máscara en 'paste'\n",
        "    mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n",
        "\n",
        "    # Crear una imagen del color del segmento que tenga las dimensiones correctas\n",
        "    color_image = Image.new(\"RGB\", segmented_image.size, color=tuple(color.astype(int)))\n",
        "\n",
        "    # Pegar usando la máscara para aplicar solo este segmento\n",
        "    segmented_image.paste(color_image, (0,0), mask=mask_image)\n",
        "\n",
        "    # Dibujar el nombre de la clase en la posición inicial del segmento\n",
        "    draw = ImageDraw.Draw(segmented_image)\n",
        "    where = np.where(mask)\n",
        "    if where[0].size > 0 and where[1].size > 0:\n",
        "        x, y = np.min(where[1]), np.min(where[0])\n",
        "        draw.text((x, y), class_name, fill='white')\n",
        "\n",
        "# Mostrar la imagen original y la segmentada\n",
        "plt.figure(figsize=(30, 15))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmented_image)\n",
        "plt.title('Imagen Segmentada con Etiquetas')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mxli-aL0ctl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la imagen\n",
        "img_path = \"mza2.jpg\"\n",
        "image = Image.open(img_path)\n",
        "\n",
        "# Cargar el modelo y el extractor de características\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "\n",
        "# Preparar la imagen para el modelo\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Paso forward\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Postprocesamiento\n",
        "processed_sizes = torch.as_tensor(inputs[\"pixel_values\"].shape[-2:]).unsqueeze(0)\n",
        "result = feature_extractor.post_process_panoptic(outputs, processed_sizes, threshold=0.85)[0]\n",
        "\n",
        "# Convertir la segmentación a numpy\n",
        "panoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\n",
        "panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
        "panoptic_seg_id = rgb_to_id(panoptic_seg)\n",
        "\n",
        "# Preparar la paleta de colores\n",
        "palette = itertools.cycle(sns.color_palette())\n",
        "\n",
        "# Crear la imagen segmentada\n",
        "segmented_image = Image.fromarray(np.zeros_like(panoptic_seg, dtype=np.uint8))\n",
        "draw = ImageDraw.Draw(segmented_image)\n",
        "\n",
        "\n",
        "# Ajusta el bucle para dibujar segmentos y etiquetas correctamente\n",
        "for segment_info in result[\"segments_info\"]:\n",
        "    class_id = segment_info[\"category_id\"]\n",
        "    class_name = COCO_LABELS.get(class_id, 'Desconocido')  # 'Desconocido' si el ID no está en el diccionario\n",
        "    id = segment_info[\"id\"]\n",
        "\n",
        "    # Generar la máscara para este segmento específico\n",
        "    mask = panoptic_seg_id == id\n",
        "    color = np.array(next(palette)) * 255  # Convertir el color a un array de numpy adecuado\n",
        "\n",
        "    # Convertir la máscara a una imagen de PIL para usarla como máscara en 'paste'\n",
        "    mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n",
        "\n",
        "    # Crear una imagen del color del segmento que tenga las dimensiones correctas\n",
        "    color_image = Image.new(\"RGB\", segmented_image.size, color=tuple(color.astype(int)))\n",
        "\n",
        "    # Pegar usando la máscara para aplicar solo este segmento\n",
        "    segmented_image.paste(color_image, (0,0), mask=mask_image)\n",
        "\n",
        "    # Dibujar el nombre de la clase en la posición inicial del segmento\n",
        "    draw = ImageDraw.Draw(segmented_image)\n",
        "    where = np.where(mask)\n",
        "    if where[0].size > 0 and where[1].size > 0:\n",
        "        x, y = np.min(where[1]), np.min(where[0])\n",
        "        draw.text((x, y), class_name, fill='white')\n",
        "\n",
        "# Mostrar la imagen original y la segmentada\n",
        "plt.figure(figsize=(30, 15))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmented_image)\n",
        "plt.title('Imagen Segmentada con Etiquetas')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FaDlLIgX2KRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la imagen\n",
        "img_path = \"sj1.jpg\"\n",
        "image = Image.open(img_path)\n",
        "\n",
        "# Cargar el modelo y el extractor de características\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "\n",
        "# Preparar la imagen para el modelo\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Paso forward\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Postprocesamiento\n",
        "processed_sizes = torch.as_tensor(inputs[\"pixel_values\"].shape[-2:]).unsqueeze(0)\n",
        "result = feature_extractor.post_process_panoptic(outputs, processed_sizes, threshold=0.85)[0]\n",
        "\n",
        "# Convertir la segmentación a numpy\n",
        "panoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\n",
        "panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
        "panoptic_seg_id = rgb_to_id(panoptic_seg)\n",
        "\n",
        "# Preparar la paleta de colores\n",
        "palette = itertools.cycle(sns.color_palette())\n",
        "\n",
        "# Crear la imagen segmentada\n",
        "segmented_image = Image.fromarray(np.zeros_like(panoptic_seg, dtype=np.uint8))\n",
        "draw = ImageDraw.Draw(segmented_image)\n",
        "\n",
        "\n",
        "# Ajusta el bucle para dibujar segmentos y etiquetas correctamente\n",
        "for segment_info in result[\"segments_info\"]:\n",
        "    class_id = segment_info[\"category_id\"]\n",
        "    class_name = COCO_LABELS.get(class_id, 'Desconocido')  # 'Desconocido' si el ID no está en el diccionario\n",
        "    id = segment_info[\"id\"]\n",
        "\n",
        "    # Generar la máscara para este segmento específico\n",
        "    mask = panoptic_seg_id == id\n",
        "    color = np.array(next(palette)) * 255  # Convertir el color a un array de numpy adecuado\n",
        "\n",
        "    # Convertir la máscara a una imagen de PIL para usarla como máscara en 'paste'\n",
        "    mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n",
        "\n",
        "    # Crear una imagen del color del segmento que tenga las dimensiones correctas\n",
        "    color_image = Image.new(\"RGB\", segmented_image.size, color=tuple(color.astype(int)))\n",
        "\n",
        "    # Pegar usando la máscara para aplicar solo este segmento\n",
        "    segmented_image.paste(color_image, (0,0), mask=mask_image)\n",
        "\n",
        "    # Dibujar el nombre de la clase en la posición inicial del segmento\n",
        "    draw = ImageDraw.Draw(segmented_image)\n",
        "    where = np.where(mask)\n",
        "    if where[0].size > 0 and where[1].size > 0:\n",
        "        x, y = np.min(where[1]), np.min(where[0])\n",
        "        draw.text((x, y), class_name, fill='white')\n",
        "\n",
        "# Mostrar la imagen original y la segmentada\n",
        "plt.figure(figsize=(30, 15))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmented_image)\n",
        "plt.title('Imagen Segmentada con Etiquetas')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g37aczaV2hKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "Al probar los distintos modelos de segmentación se obtuvieron diferentes resultados.\n",
        "\n",
        "De acuerdo a los modelos de segmentación por instancia se observan en todos buenos resultados en cuanto a la segmentación de los objetos que figuran en las imágenes pero en cuanto a mi percepción, veo que el modelo PointRed ResNet de PixelLib no solo segmenta a la perfección cada instancia de la imagen sino que tambien pinta cada uno de un color distinto, que sirve mucho para diferenciar cada objeto, y además agrega la etiqueta de cada clase a la salida.\n",
        "\n",
        "El modelo de YOLO realiza muy buena segmentación pero no pinta de color las instancias y tampoco agrega etiqueta de la clase.\n",
        "\n",
        "El modelo Mask R-CNN de OpenCV realiza una buena segmentación en la primera imagen pero en la segunda no logra segmentar bien por ejemplo a la moto.\n",
        "\n",
        "El modelo de segmentación semántica DeepLabV3 de Torchvision logra realizar una segmentación muy precisa de cada objeto pero por ejemplo en la segunda imagen segmenta parte de un árbol que luego no se logra entender bien de que se trata en el resultado final.\n",
        "\n",
        "No pude hacer funcionar el modelo MMSegmentation por problemas con la instalación, chequearé si puedo solucionar este tema.\n",
        "\n",
        "Por último en el modelo de segmentación Panoptic DETR ResNet50 de Transformers se logra ver la combinación entre la segmentacion semántica y de instancia, logrando observar una panorama general de la escena con las etiquetas de clase. Un detalle que veo es que por ahí no se logra apreciar bien del todo lo que está pasando cuando hay algun árbol entre medio o porque no llega a realizar una segmentación de los edificios, pintando todo de un mismo color, agregando también parte del cielo."
      ],
      "metadata": {
        "id": "Ce7CJG_F22eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 10\n",
        "\n",
        "Con el mismo video usado en el ejercicio 4, realice un seguimiento de objetos con el código propuesto en la sección de ejemplos prácticos."
      ],
      "metadata": {
        "id": "yNUOtm09x40q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "\n",
        "url = 'video_600.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "1fy6SQm5wv6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision ultralytics\n",
        "\n",
        "# Importar las librerías necesarias\n",
        "import supervision as sv  # Librería para el seguimiento de objetos\n",
        "from ultralytics import YOLO  # Librería para la detección de objetos con YOLO\n",
        "import numpy as np  # Librería para operaciones matemáticas y manejo de arrays\n",
        "\n",
        "# Definir rutas de los videos de entrada y salida, y el nombre del modelo\n",
        "SOURCE_VIDEO_PATH = \"video_600.mp4\"  # Ruta al video de entrada\n",
        "TARGET_VIDEO_PATH = \"video_600_tracking.mp4\"  # Ruta donde se guardará el video con seguimiento de objetos\n",
        "MODEL_NAME = \"yolov8x.pt\"  # Nombre del modelo preentrenado de YOLO a utilizar\n",
        "\n",
        "# Inicializar el modelo de detección y el rastreador de objetos\n",
        "model = YOLO(MODEL_NAME)  # Cargar el modelo de YOLO\n",
        "tracker = sv.ByteTrack()  # Inicializar ByteTrack para el seguimiento de objetos\n",
        "\n",
        "# Inicializar los anotadores para las cajas delimitadoras y las etiquetas\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator()  # Para dibujar cajas delimitadoras\n",
        "label_annotator = sv.LabelAnnotator()  # Para dibujar etiquetas (IDs de seguimiento)\n",
        "\n",
        "# Definir la función de callback que procesa cada frame\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    results = model(frame)[0]  # Detectar objetos en el frame actual con YOLO\n",
        "    detections = sv.Detections.from_ultralytics(results)  # Convertir resultados a formato de supervision\n",
        "    detections = tracker.update_with_detections(detections)  # Actualizar el estado del rastreador con las detecciones\n",
        "\n",
        "    labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]  # Crear etiquetas con los IDs de seguimiento\n",
        "\n",
        "    # Anotar el frame con bounding boxes y etiquetas\n",
        "    annotated_frame = bounding_box_annotator.annotate(\n",
        "        scene=frame.copy(), detections=detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame, detections=detections, labels=labels)\n",
        "    return annotated_frame  # Devolver el frame anotado\n",
        "\n",
        "# Procesar el video: leer, aplicar callback a cada frame y guardar el resultado\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,  # Ruta del video original\n",
        "    target_path=TARGET_VIDEO_PATH,  # Ruta del video resultante\n",
        "    callback=callback  # Función de callback para procesar cada frame\n",
        ")\n"
      ],
      "metadata": {
        "id": "8moWo5Mmx3t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'video_600_tracking.mp4'\n",
        "video = media.read_video(url)\n",
        "media.show_video(video)"
      ],
      "metadata": {
        "id": "yvc5Sw6V0cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede ver que este modelo segmenta mucho mejor que las funciones utilizadas en el ejercicio 4. Se podrían mejorar aplicando distintos filtros o cambiando los diferentes parámetros de las funciones de OpenCV utilizadas."
      ],
      "metadata": {
        "id": "armtBKkq3ivU"
      }
    }
  ]
}